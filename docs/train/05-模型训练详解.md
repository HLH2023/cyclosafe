# æ¨¡å‹è®­ç»ƒè¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæµç¨‹ã€ç‰¹å¾å·¥ç¨‹ã€ç®—æ³•é€‰æ‹©å’Œè¯„ä¼°æ–¹æ³•ã€‚

---

## ç›®å½•

1. [æ•°æ®å‡†å¤‡](#1-æ•°æ®å‡†å¤‡)
2. [ç‰¹å¾å·¥ç¨‹](#2-ç‰¹å¾å·¥ç¨‹)
3. [æ¨¡å‹é€‰æ‹©](#3-æ¨¡å‹é€‰æ‹©)
4. [è®­ç»ƒæµç¨‹](#4-è®­ç»ƒæµç¨‹)
5. [æ¨¡å‹è¯„ä¼°](#5-æ¨¡å‹è¯„ä¼°)
6. [è¶…å‚æ•°è°ƒä¼˜](#6-è¶…å‚æ•°è°ƒä¼˜)
7. [æ•°æ®å¢å¼º](#7-æ•°æ®å¢å¼º)

---

## 1. æ•°æ®å‡†å¤‡

### 1.1 æ•°æ®åŠ è½½

ä»MySQLæ•°æ®åº“åŠ è½½è®­ç»ƒæ•°æ®ï¼š

```python
# training/data_loader.py
import pandas as pd
from sqlalchemy import create_engine
from typing import Tuple, List

class DataLoader:
    """æ•°æ®åŠ è½½å™¨"""

    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)

    def load_samples_metadata(self) -> pd.DataFrame:
        """åŠ è½½æ ·æœ¬å…ƒæ•°æ®"""
        query = """
        SELECT
            id, user_id, label, source,
            duration, sample_count, uploaded_at
        FROM training_samples
        WHERE processed = FALSE
        ORDER BY uploaded_at ASC
        """
        return pd.read_sql(query, self.engine)

    def load_sensor_data(self, sample_id: int) -> pd.DataFrame:
        """åŠ è½½å•ä¸ªæ ·æœ¬çš„ä¼ æ„Ÿå™¨æ•°æ®"""
        query = f"""
        SELECT
            timestamp,
            acc_x, acc_y, acc_z,
            gyro_x, gyro_y, gyro_z
        FROM sensor_data
        WHERE sample_id = {sample_id}
        ORDER BY timestamp ASC
        """
        return pd.read_sql(query, self.engine)

    def load_all_data(self) -> Tuple[List[pd.DataFrame], List[int]]:
        """åŠ è½½æ‰€æœ‰æ ·æœ¬æ•°æ®"""
        samples = self.load_samples_metadata()

        data_list = []
        labels = []

        for _, sample in samples.iterrows():
            sensor_data = self.load_sensor_data(sample['id'])

            # æ•°æ®è´¨é‡æ£€æŸ¥
            if len(sensor_data) < 50:  # è‡³å°‘1ç§’æ•°æ®
                print(f"è·³è¿‡æ ·æœ¬ {sample['id']}: æ•°æ®ä¸è¶³")
                continue

            data_list.append(sensor_data)
            labels.append(1 if sample['label'] == 'fall' else 0)

        return data_list, labels
```

### 1.2 æ•°æ®æ¸…æ´—

```python
def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ¸…æ´—ä¼ æ„Ÿå™¨æ•°æ®"""

    # 1. ç§»é™¤å¼‚å¸¸å€¼ï¼ˆ3-sigmaåŸåˆ™ï¼‰
    for col in df.columns:
        if col == 'timestamp':
            continue

        mean = df[col].mean()
        std = df[col].std()

        # è¶…è¿‡3å€æ ‡å‡†å·®çš„è§†ä¸ºå¼‚å¸¸
        df = df[(df[col] >= mean - 3*std) & (df[col] <= mean + 3*std)]

    # 2. å¡«å……ç¼ºå¤±å€¼ï¼ˆçº¿æ€§æ’å€¼ï¼‰
    df = df.interpolate(method='linear')

    # 3. é‡é‡‡æ ·åˆ°å›ºå®šé¢‘ç‡ï¼ˆ50Hzï¼‰
    df = df.set_index('timestamp')
    df = df.resample('20ms').mean()  # 20ms = 50Hz
    df = df.interpolate()

    return df.reset_index()
```

### 1.3 æ•°æ®ç»Ÿè®¡

```python
def analyze_dataset(data_list: List[pd.DataFrame], labels: List[int]):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""

    print("=" * 50)
    print("æ•°æ®é›†ç»Ÿè®¡")
    print("=" * 50)

    # æ ·æœ¬æ•°é‡
    total_samples = len(labels)
    fall_samples = sum(labels)
    normal_samples = total_samples - fall_samples

    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"æ‘”å€’æ ·æœ¬: {fall_samples} ({fall_samples/total_samples*100:.1f}%)")
    print(f"æ­£å¸¸æ ·æœ¬: {normal_samples} ({normal_samples/total_samples*100:.1f}%)")

    # æ ·æœ¬é•¿åº¦ç»Ÿè®¡
    lengths = [len(df) for df in data_list]
    print(f"\næ ·æœ¬é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡: {np.mean(lengths):.0f} ä¸ªæ•°æ®ç‚¹")
    print(f"  æœ€å°: {np.min(lengths)} ä¸ªæ•°æ®ç‚¹")
    print(f"  æœ€å¤§: {np.max(lengths)} ä¸ªæ•°æ®ç‚¹")

    # ç±»åˆ«å¹³è¡¡æ€§
    ratio = fall_samples / normal_samples if normal_samples > 0 else 0
    print(f"\nç±»åˆ«æ¯”ä¾‹: 1:{ratio:.1f}")

    if ratio < 0.1:
        print("âš ï¸ è­¦å‘Š: æ‘”å€’æ ·æœ¬è¿‡å°‘ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ‘”å€’æ•°æ®")

    print("=" * 50)
```

---

## 2. ç‰¹å¾å·¥ç¨‹

### 2.1 æ»‘åŠ¨çª—å£

```python
class SlidingWindowExtractor:
    """æ»‘åŠ¨çª—å£ç‰¹å¾æå–å™¨"""

    def __init__(self, window_size=100, overlap=50):
        """
        Args:
            window_size: çª—å£å¤§å°ï¼ˆæ•°æ®ç‚¹æ•°ï¼‰
            overlap: é‡å æ•°æ®ç‚¹æ•°
        """
        self.window_size = window_size
        self.overlap = overlap
        self.step = window_size - overlap

    def create_windows(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """åˆ›å»ºæ»‘åŠ¨çª—å£"""
        windows = []

        for i in range(0, len(df) - self.window_size + 1, self.step):
            window = df.iloc[i:i + self.window_size]
            windows.append(window)

        return windows
```

### 2.2 æ—¶åŸŸç‰¹å¾

```python
import numpy as np
from scipy import stats

def extract_time_features(signal: np.ndarray) -> List[float]:
    """æå–æ—¶åŸŸç‰¹å¾"""

    features = []

    # 1. åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
    features.append(np.mean(signal))              # å‡å€¼
    features.append(np.std(signal))               # æ ‡å‡†å·®
    features.append(np.min(signal))               # æœ€å°å€¼
    features.append(np.max(signal))               # æœ€å¤§å€¼
    features.append(np.ptp(signal))               # æå·® (max - min)

    # 2. é«˜çº§ç»Ÿè®¡ç‰¹å¾
    features.append(np.median(signal))            # ä¸­ä½æ•°
    features.append(np.percentile(signal, 25))    # 25%åˆ†ä½æ•°
    features.append(np.percentile(signal, 75))    # 75%åˆ†ä½æ•°
    features.append(stats.iqr(signal))            # å››åˆ†ä½è·

    # 3. ä¿¡å·ç‰¹å¾
    features.append(np.sqrt(np.mean(signal**2)))  # RMS (å‡æ–¹æ ¹)
    features.append(stats.skew(signal))           # ååº¦ (åˆ†å¸ƒå¯¹ç§°æ€§)
    features.append(stats.kurtosis(signal))       # å³°åº¦ (åˆ†å¸ƒå°–é”ç¨‹åº¦)

    # 4. èƒ½é‡ç‰¹å¾
    features.append(np.sum(signal**2))            # èƒ½é‡
    features.append(np.mean(np.abs(signal)))      # å¹³å‡ç»å¯¹å€¼

    # 5. å˜åŒ–ç‡ç‰¹å¾
    diff = np.diff(signal)
    features.append(np.mean(np.abs(diff)))        # å¹³å‡å˜åŒ–ç‡
    features.append(np.max(np.abs(diff)))         # æœ€å¤§å˜åŒ–ç‡

    # 6. è¿‡é›¶ç‡
    zero_crossings = np.sum(np.diff(np.sign(signal)) != 0)
    features.append(zero_crossings / len(signal))

    return features

# æ¯ä¸ªè½´17ä¸ªç‰¹å¾ Ã— 6ä¸ªè½´ = 102ç»´æ—¶åŸŸç‰¹å¾
```

### 2.3 é¢‘åŸŸç‰¹å¾

```python
from scipy.fft import fft, fftfreq

def extract_frequency_features(signal: np.ndarray, sampling_rate=50) -> List[float]:
    """æå–é¢‘åŸŸç‰¹å¾ï¼ˆFFTï¼‰"""

    features = []

    # 1. è®¡ç®—FFT
    fft_vals = np.abs(fft(signal))
    fft_vals = fft_vals[:len(fft_vals)//2]  # åªå–æ­£é¢‘ç‡éƒ¨åˆ†
    freqs = fftfreq(len(signal), 1/sampling_rate)[:len(fft_vals)]

    # 2. é¢‘è°±èƒ½é‡
    features.append(np.sum(fft_vals))             # æ€»èƒ½é‡
    features.append(np.max(fft_vals))             # å³°å€¼èƒ½é‡

    # 3. ä¸»é¢‘ç‡
    peak_freq_idx = np.argmax(fft_vals)
    features.append(freqs[peak_freq_idx])         # ä¸»é¢‘ç‡

    # 4. é¢‘è°±è´¨å¿ƒ
    spectral_centroid = np.sum(freqs * fft_vals) / np.sum(fft_vals)
    features.append(spectral_centroid)

    # 5. é¢‘è°±ç†µ
    normalized_spectrum = fft_vals / np.sum(fft_vals)
    spectral_entropy = -np.sum(normalized_spectrum * np.log2(normalized_spectrum + 1e-10))
    features.append(spectral_entropy)

    # 6. é¢‘å¸¦èƒ½é‡ï¼ˆåˆ†5ä¸ªé¢‘å¸¦ï¼‰
    band_edges = [0, 5, 10, 15, 20, 25]  # Hz
    for i in range(len(band_edges) - 1):
        band_mask = (freqs >= band_edges[i]) & (freqs < band_edges[i+1])
        band_energy = np.sum(fft_vals[band_mask])
        features.append(band_energy)

    return features

# æ¯ä¸ªè½´10ä¸ªç‰¹å¾ Ã— 6ä¸ªè½´ = 60ç»´é¢‘åŸŸç‰¹å¾
```

### 2.4 åˆæˆç‰¹å¾

```python
def extract_composite_features(window: pd.DataFrame) -> List[float]:
    """æå–åˆæˆç‰¹å¾"""

    features = []

    # 1. åŠ é€Ÿåº¦æ€»é‡
    acc_magnitude = np.sqrt(
        window['acc_x']**2 +
        window['acc_y']**2 +
        window['acc_z']**2
    )

    features.append(np.mean(acc_magnitude))
    features.append(np.std(acc_magnitude))
    features.append(np.max(acc_magnitude))
    features.append(np.min(acc_magnitude))

    # 2. è§’é€Ÿåº¦æ€»é‡
    gyro_magnitude = np.sqrt(
        window['gyro_x']**2 +
        window['gyro_y']**2 +
        window['gyro_z']**2
    )

    features.append(np.mean(gyro_magnitude))
    features.append(np.std(gyro_magnitude))
    features.append(np.max(gyro_magnitude))
    features.append(np.min(gyro_magnitude))

    # 3. åŠ é€Ÿåº¦å’Œè§’é€Ÿåº¦çš„ç›¸å…³æ€§
    correlation = np.corrcoef(acc_magnitude, gyro_magnitude)[0, 1]
    features.append(correlation)

    # 4. å§¿æ€å˜åŒ–ä¼°è®¡
    tilt_change = np.std(window['acc_z'])
    features.append(tilt_change)

    # 5. å†²å‡»æ£€æµ‹
    acc_diff = np.diff(acc_magnitude)
    max_impact = np.max(np.abs(acc_diff))
    features.append(max_impact)

    return features

# 11ç»´åˆæˆç‰¹å¾
```

### 2.5 å®Œæ•´ç‰¹å¾æå–å™¨

```python
class FeatureExtractor:
    """å®Œæ•´ç‰¹å¾æå–å™¨"""

    def __init__(self, window_size=100, overlap=50, sampling_rate=50):
        self.window_size = window_size
        self.overlap = overlap
        self.step = window_size - overlap
        self.sampling_rate = sampling_rate

    def extract_features(self, sensor_data: pd.DataFrame) -> np.ndarray:
        """æå–æ‰€æœ‰ç‰¹å¾"""

        features_list = []

        # æ»‘åŠ¨çª—å£
        for i in range(0, len(sensor_data) - self.window_size + 1, self.step):
            window = sensor_data.iloc[i:i + self.window_size]

            # æå–çª—å£ç‰¹å¾
            window_features = self._extract_window_features(window)
            features_list.append(window_features)

        return np.array(features_list)

    def _extract_window_features(self, window: pd.DataFrame) -> List[float]:
        """æå–å•ä¸ªçª—å£çš„æ‰€æœ‰ç‰¹å¾"""

        all_features = []

        # å¯¹æ¯ä¸ªä¼ æ„Ÿå™¨è½´æå–æ—¶åŸŸå’Œé¢‘åŸŸç‰¹å¾
        sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']

        for col in sensor_cols:
            signal = window[col].values

            # æ—¶åŸŸç‰¹å¾ (17ç»´)
            time_features = extract_time_features(signal)
            all_features.extend(time_features)

            # é¢‘åŸŸç‰¹å¾ (10ç»´)
            freq_features = extract_frequency_features(signal, self.sampling_rate)
            all_features.extend(freq_features)

        # åˆæˆç‰¹å¾ (11ç»´)
        composite_features = extract_composite_features(window)
        all_features.extend(composite_features)

        return all_features

# æ€»ç‰¹å¾æ•°: 6è½´ Ã— (17æ—¶åŸŸ + 10é¢‘åŸŸ) + 11åˆæˆ = 173ç»´
```

---

## 3. æ¨¡å‹é€‰æ‹©

### 3.1 ç®—æ³•å¯¹æ¯”

| ç®—æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | è®­ç»ƒæ—¶é—´ | å‡†ç¡®ç‡ | æ¨èåº¦ |
|-----|------|------|---------|--------|-------|
| **Random Forest** | æ˜“ç”¨ã€å¿«é€Ÿã€å¯è§£é‡Šæ€§å¥½ | æ¨¡å‹è¾ƒå¤§ | å¿« | 90-92% | â­â­â­â­â­ |
| **XGBoost** | å‡†ç¡®ç‡é«˜ã€é˜²è¿‡æ‹Ÿåˆ | è°ƒå‚å¤æ‚ | ä¸­ç­‰ | 92-95% | â­â­â­â­ |
| **SVM** | å°æ ·æœ¬æ•ˆæœå¥½ | å¤§æ•°æ®é›†æ…¢ | æ…¢ | 88-90% | â­â­â­ |
| **Logistic Regression** | ç®€å•ã€å¿«é€Ÿ | å‡†ç¡®ç‡ä½ | å¿« | 85-87% | â­â­ |
| **CNN-LSTM** | å‡†ç¡®ç‡æœ€é«˜ | éœ€è¦å¤§é‡æ•°æ® | å¾ˆæ…¢ | 95-98% | â­â­â­â­ |

### 3.2 Random Forestï¼ˆæ¨èï¼‰

```python
from sklearn.ensemble import RandomForestClassifier

# é…ç½®
rf_config = {
    'n_estimators': 100,        # æ ‘çš„æ•°é‡
    'max_depth': 20,            # æœ€å¤§æ·±åº¦
    'min_samples_split': 5,     # åˆ†è£‚æœ€å°æ ·æœ¬æ•°
    'min_samples_leaf': 2,      # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    'max_features': 'sqrt',     # ç‰¹å¾é‡‡æ ·æ•°é‡
    'bootstrap': True,          # æ˜¯å¦Bootstrapé‡‡æ ·
    'random_state': 42,         # éšæœºç§å­
    'n_jobs': -1               # å¹¶è¡Œæ•°ï¼ˆ-1è¡¨ç¤ºå…¨éƒ¨CPUï¼‰
}

model = RandomForestClassifier(**rf_config)
```

### 3.3 XGBoostï¼ˆå¤‡é€‰ï¼‰

```python
import xgboost as xgb

# é…ç½®
xgb_config = {
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42
}

model = xgb.XGBClassifier(**xgb_config)
```

---

## 4. è®­ç»ƒæµç¨‹

### 4.1 å®Œæ•´è®­ç»ƒè„šæœ¬

```python
# training/train_model.py
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import json
from datetime import datetime

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, db_url):
        self.db_url = db_url
        self.data_loader = DataLoader(db_url)
        self.feature_extractor = FeatureExtractor(window_size=100, overlap=50)
        self.model = None

    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")

        # 1. åŠ è½½åŸå§‹æ•°æ®
        data_list, labels = self.data_loader.load_all_data()

        print(f"âœ… åŠ è½½äº† {len(data_list)} ä¸ªæ ·æœ¬")

        # 2. æ•°æ®æ¸…æ´—
        data_list = [clean_data(df) for df in data_list]

        # 3. ç‰¹å¾æå–
        print("ğŸ”§ æå–ç‰¹å¾...")
        X_list = []
        y_list = []

        for data, label in zip(data_list, labels):
            features = self.feature_extractor.extract_features(data)

            for feature_vec in features:
                X_list.append(feature_vec)
                y_list.append(label)

        X = np.array(X_list)
        y = np.array(y_list)

        print(f"âœ… æå–äº† {len(X)} ä¸ªç‰¹å¾å‘é‡ (ç»´åº¦: {X.shape[1]})")

        return X, y

    def train(self, X, y, test_size=0.2, val_size=0.1):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")

        # 1. åˆ’åˆ†æ•°æ®é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size/(1-test_size),
            random_state=42, stratify=y_temp
        )

        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

        # 2. è®­ç»ƒæ¨¡å‹
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            random_state=42,
            n_jobs=-1,
            verbose=1
        )

        self.model.fit(X_train, y_train)
        print("âœ… è®­ç»ƒå®Œæˆ")

        # 3. éªŒè¯é›†è¯„ä¼°
        val_score = self.model.score(X_val, y_val)
        print(f"ğŸ“ˆ éªŒè¯é›†å‡†ç¡®ç‡: {val_score:.3f}")

        # 4. æµ‹è¯•é›†è¯„ä¼°
        metrics = self.evaluate(X_test, y_test)

        return metrics

    def evaluate(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹"""
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°:")

        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]

        # åˆ†ç±»æŠ¥å‘Š
        print(classification_report(y_test, y_pred,
                                   target_names=['Normal', 'Fall']))

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        print("\næ··æ·†çŸ©é˜µ:")
        print(cm)

        # è®¡ç®—æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel()
        metrics = {
            'accuracy': (tp + tn) / (tp + tn + fp + fn),
            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'f1_score': 2*tp / (2*tp + fp + fn) if (2*tp + fp + fn) > 0 else 0,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn)
        }

        print(f"\nğŸ“ˆ ç»¼åˆæŒ‡æ ‡:")
        print(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
        print(f"   ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
        print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
        print(f"   F1åˆ†æ•°: {metrics['f1_score']:.3f}")

        return metrics

    def save_model(self, output_dir='./models'):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"{output_dir}/fall_detection_rf_{timestamp}.pkl"

        joblib.dump(self.model, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

        return model_path

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    trainer = ModelTrainer(
        db_url='mysql+pymysql://root:password@localhost/fall_detection_training'
    )

    # å‡†å¤‡æ•°æ®
    X, y = trainer.prepare_data()

    # è®­ç»ƒæ¨¡å‹
    metrics = trainer.train(X, y)

    # ä¿å­˜æ¨¡å‹
    model_path = trainer.save_model()

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
```

---

## 5. æ¨¡å‹è¯„ä¼°

### 5.1 äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

def cross_validation(model, X, y, cv=5):
    """KæŠ˜äº¤å‰éªŒè¯"""

    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)

    scores = cross_val_score(model, X, y, cv=skf, scoring='f1')

    print(f"äº¤å‰éªŒè¯ F1åˆ†æ•°: {scores}")
    print(f"å¹³å‡: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")

    return scores
```

### 5.2 ROCæ›²çº¿

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

def plot_roc_curve(y_test, y_proba):
    """ç»˜åˆ¶ROCæ›²çº¿"""

    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')
    plt.show()

    return roc_auc
```

### 5.3 ç‰¹å¾é‡è¦æ€§

```python
def plot_feature_importance(model, feature_names, top_n=20):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""

    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    plt.figure(figsize=(10, 6))
    plt.title(f'Top {top_n} Feature Importances')
    plt.barh(range(top_n), importances[indices])
    plt.yticks(range(top_n), [feature_names[i] for i in indices])
    plt.xlabel('Importance')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
```

---

## 6. è¶…å‚æ•°è°ƒä¼˜

### 6.1 ç½‘æ ¼æœç´¢

```python
from sklearn.model_selection import GridSearchCV

def grid_search_tuning(X_train, y_train):
    """ç½‘æ ¼æœç´¢è°ƒä¼˜"""

    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    rf = RandomForestClassifier(random_state=42, n_jobs=-1)

    grid_search = GridSearchCV(
        rf, param_grid, cv=5,
        scoring='f1', verbose=2, n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³F1åˆ†æ•°: {grid_search.best_score_:.3f}")

    return grid_search.best_estimator_
```

---

## 7. æ•°æ®å¢å¼º

### 7.1 æ—¶é—´åºåˆ—å¢å¼º

```python
def augment_data(sensor_data, label, n_augmentations=5):
    """æ•°æ®å¢å¼º"""

    augmented_data = [sensor_data]  # åŸå§‹æ•°æ®

    for _ in range(n_augmentations):
        # 1. æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.1, sensor_data.shape)
        noisy_data = sensor_data + noise

        # 2. ç¼©æ”¾
        scale_factor = np.random.uniform(0.9, 1.1)
        scaled_data = sensor_data * scale_factor

        # 3. æ—¶é—´åç§»
        shift = np.random.randint(-5, 5)
        shifted_data = np.roll(sensor_data, shift, axis=0)

        augmented_data.extend([noisy_data, scaled_data, shifted_data])

    return augmented_data
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ã€‚å…³é”®è¦ç‚¹ï¼š

1. âœ… **æ•°æ®å‡†å¤‡**: åŠ è½½ã€æ¸…æ´—ã€ç»Ÿè®¡åˆ†æ
2. âœ… **ç‰¹å¾å·¥ç¨‹**: æ—¶åŸŸã€é¢‘åŸŸã€åˆæˆç‰¹å¾ï¼ˆ173ç»´ï¼‰
3. âœ… **æ¨¡å‹é€‰æ‹©**: Random Forestï¼ˆæ¨èï¼‰
4. âœ… **è®­ç»ƒæµç¨‹**: å®Œæ•´çš„è®­ç»ƒè„šæœ¬
5. âœ… **æ¨¡å‹è¯„ä¼°**: å¤šç§è¯„ä¼°æ–¹æ³•
6. âœ… **è¶…å‚æ•°è°ƒä¼˜**: ç½‘æ ¼æœç´¢
7. âœ… **æ•°æ®å¢å¼º**: æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›

**ä¸‹ä¸€æ­¥:** é˜…è¯» `06-æ¨¡å‹éƒ¨ç½².md` äº†è§£å¦‚ä½•å°†æ¨¡å‹éƒ¨ç½²åˆ°å°ç¨‹åºã€‚
