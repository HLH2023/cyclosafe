# 模型训练详解

本文档详细说明机器学习模型的训练流程、特征工程、算法选择和评估方法。

---

## 目录

1. [数据准备](#1-数据准备)
2. [特征工程](#2-特征工程)
3. [模型选择](#3-模型选择)
4. [训练流程](#4-训练流程)
5. [模型评估](#5-模型评估)
6. [超参数调优](#6-超参数调优)
7. [数据增强](#7-数据增强)

---

## 1. 数据准备

### 1.1 数据加载

从MySQL数据库加载训练数据：

```python
# training/data_loader.py
import pandas as pd
from sqlalchemy import create_engine
from typing import Tuple, List

class DataLoader:
    """数据加载器"""

    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)

    def load_samples_metadata(self) -> pd.DataFrame:
        """加载样本元数据"""
        query = """
        SELECT
            id, user_id, label, source,
            duration, sample_count, uploaded_at
        FROM training_samples
        WHERE processed = FALSE
        ORDER BY uploaded_at ASC
        """
        return pd.read_sql(query, self.engine)

    def load_sensor_data(self, sample_id: int) -> pd.DataFrame:
        """加载单个样本的传感器数据"""
        query = f"""
        SELECT
            timestamp,
            acc_x, acc_y, acc_z,
            gyro_x, gyro_y, gyro_z
        FROM sensor_data
        WHERE sample_id = {sample_id}
        ORDER BY timestamp ASC
        """
        return pd.read_sql(query, self.engine)

    def load_all_data(self) -> Tuple[List[pd.DataFrame], List[int]]:
        """加载所有样本数据"""
        samples = self.load_samples_metadata()

        data_list = []
        labels = []

        for _, sample in samples.iterrows():
            sensor_data = self.load_sensor_data(sample['id'])

            # 数据质量检查
            if len(sensor_data) < 50:  # 至少1秒数据
                print(f"跳过样本 {sample['id']}: 数据不足")
                continue

            data_list.append(sensor_data)
            labels.append(1 if sample['label'] == 'fall' else 0)

        return data_list, labels
```

### 1.2 数据清洗

```python
def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """清洗传感器数据"""

    # 1. 移除异常值（3-sigma原则）
    for col in df.columns:
        if col == 'timestamp':
            continue

        mean = df[col].mean()
        std = df[col].std()

        # 超过3倍标准差的视为异常
        df = df[(df[col] >= mean - 3*std) & (df[col] <= mean + 3*std)]

    # 2. 填充缺失值（线性插值）
    df = df.interpolate(method='linear')

    # 3. 重采样到固定频率（50Hz）
    df = df.set_index('timestamp')
    df = df.resample('20ms').mean()  # 20ms = 50Hz
    df = df.interpolate()

    return df.reset_index()
```

### 1.3 数据统计

```python
def analyze_dataset(data_list: List[pd.DataFrame], labels: List[int]):
    """分析数据集统计信息"""

    print("=" * 50)
    print("数据集统计")
    print("=" * 50)

    # 样本数量
    total_samples = len(labels)
    fall_samples = sum(labels)
    normal_samples = total_samples - fall_samples

    print(f"总样本数: {total_samples}")
    print(f"摔倒样本: {fall_samples} ({fall_samples/total_samples*100:.1f}%)")
    print(f"正常样本: {normal_samples} ({normal_samples/total_samples*100:.1f}%)")

    # 样本长度统计
    lengths = [len(df) for df in data_list]
    print(f"\n样本长度统计:")
    print(f"  平均: {np.mean(lengths):.0f} 个数据点")
    print(f"  最小: {np.min(lengths)} 个数据点")
    print(f"  最大: {np.max(lengths)} 个数据点")

    # 类别平衡性
    ratio = fall_samples / normal_samples if normal_samples > 0 else 0
    print(f"\n类别比例: 1:{ratio:.1f}")

    if ratio < 0.1:
        print("⚠️ 警告: 摔倒样本过少，建议收集更多摔倒数据")

    print("=" * 50)
```

---

## 2. 特征工程

### 2.1 滑动窗口

```python
class SlidingWindowExtractor:
    """滑动窗口特征提取器"""

    def __init__(self, window_size=100, overlap=50):
        """
        Args:
            window_size: 窗口大小（数据点数）
            overlap: 重叠数据点数
        """
        self.window_size = window_size
        self.overlap = overlap
        self.step = window_size - overlap

    def create_windows(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """创建滑动窗口"""
        windows = []

        for i in range(0, len(df) - self.window_size + 1, self.step):
            window = df.iloc[i:i + self.window_size]
            windows.append(window)

        return windows
```

### 2.2 时域特征

```python
import numpy as np
from scipy import stats

def extract_time_features(signal: np.ndarray) -> List[float]:
    """提取时域特征"""

    features = []

    # 1. 基本统计特征
    features.append(np.mean(signal))              # 均值
    features.append(np.std(signal))               # 标准差
    features.append(np.min(signal))               # 最小值
    features.append(np.max(signal))               # 最大值
    features.append(np.ptp(signal))               # 极差 (max - min)

    # 2. 高级统计特征
    features.append(np.median(signal))            # 中位数
    features.append(np.percentile(signal, 25))    # 25%分位数
    features.append(np.percentile(signal, 75))    # 75%分位数
    features.append(stats.iqr(signal))            # 四分位距

    # 3. 信号特征
    features.append(np.sqrt(np.mean(signal**2)))  # RMS (均方根)
    features.append(stats.skew(signal))           # 偏度 (分布对称性)
    features.append(stats.kurtosis(signal))       # 峰度 (分布尖锐程度)

    # 4. 能量特征
    features.append(np.sum(signal**2))            # 能量
    features.append(np.mean(np.abs(signal)))      # 平均绝对值

    # 5. 变化率特征
    diff = np.diff(signal)
    features.append(np.mean(np.abs(diff)))        # 平均变化率
    features.append(np.max(np.abs(diff)))         # 最大变化率

    # 6. 过零率
    zero_crossings = np.sum(np.diff(np.sign(signal)) != 0)
    features.append(zero_crossings / len(signal))

    return features

# 每个轴17个特征 × 6个轴 = 102维时域特征
```

### 2.3 频域特征

```python
from scipy.fft import fft, fftfreq

def extract_frequency_features(signal: np.ndarray, sampling_rate=50) -> List[float]:
    """提取频域特征（FFT）"""

    features = []

    # 1. 计算FFT
    fft_vals = np.abs(fft(signal))
    fft_vals = fft_vals[:len(fft_vals)//2]  # 只取正频率部分
    freqs = fftfreq(len(signal), 1/sampling_rate)[:len(fft_vals)]

    # 2. 频谱能量
    features.append(np.sum(fft_vals))             # 总能量
    features.append(np.max(fft_vals))             # 峰值能量

    # 3. 主频率
    peak_freq_idx = np.argmax(fft_vals)
    features.append(freqs[peak_freq_idx])         # 主频率

    # 4. 频谱质心
    spectral_centroid = np.sum(freqs * fft_vals) / np.sum(fft_vals)
    features.append(spectral_centroid)

    # 5. 频谱熵
    normalized_spectrum = fft_vals / np.sum(fft_vals)
    spectral_entropy = -np.sum(normalized_spectrum * np.log2(normalized_spectrum + 1e-10))
    features.append(spectral_entropy)

    # 6. 频带能量（分5个频带）
    band_edges = [0, 5, 10, 15, 20, 25]  # Hz
    for i in range(len(band_edges) - 1):
        band_mask = (freqs >= band_edges[i]) & (freqs < band_edges[i+1])
        band_energy = np.sum(fft_vals[band_mask])
        features.append(band_energy)

    return features

# 每个轴10个特征 × 6个轴 = 60维频域特征
```

### 2.4 合成特征

```python
def extract_composite_features(window: pd.DataFrame) -> List[float]:
    """提取合成特征"""

    features = []

    # 1. 加速度总量
    acc_magnitude = np.sqrt(
        window['acc_x']**2 +
        window['acc_y']**2 +
        window['acc_z']**2
    )

    features.append(np.mean(acc_magnitude))
    features.append(np.std(acc_magnitude))
    features.append(np.max(acc_magnitude))
    features.append(np.min(acc_magnitude))

    # 2. 角速度总量
    gyro_magnitude = np.sqrt(
        window['gyro_x']**2 +
        window['gyro_y']**2 +
        window['gyro_z']**2
    )

    features.append(np.mean(gyro_magnitude))
    features.append(np.std(gyro_magnitude))
    features.append(np.max(gyro_magnitude))
    features.append(np.min(gyro_magnitude))

    # 3. 加速度和角速度的相关性
    correlation = np.corrcoef(acc_magnitude, gyro_magnitude)[0, 1]
    features.append(correlation)

    # 4. 姿态变化估计
    tilt_change = np.std(window['acc_z'])
    features.append(tilt_change)

    # 5. 冲击检测
    acc_diff = np.diff(acc_magnitude)
    max_impact = np.max(np.abs(acc_diff))
    features.append(max_impact)

    return features

# 11维合成特征
```

### 2.5 完整特征提取器

```python
class FeatureExtractor:
    """完整特征提取器"""

    def __init__(self, window_size=100, overlap=50, sampling_rate=50):
        self.window_size = window_size
        self.overlap = overlap
        self.step = window_size - overlap
        self.sampling_rate = sampling_rate

    def extract_features(self, sensor_data: pd.DataFrame) -> np.ndarray:
        """提取所有特征"""

        features_list = []

        # 滑动窗口
        for i in range(0, len(sensor_data) - self.window_size + 1, self.step):
            window = sensor_data.iloc[i:i + self.window_size]

            # 提取窗口特征
            window_features = self._extract_window_features(window)
            features_list.append(window_features)

        return np.array(features_list)

    def _extract_window_features(self, window: pd.DataFrame) -> List[float]:
        """提取单个窗口的所有特征"""

        all_features = []

        # 对每个传感器轴提取时域和频域特征
        sensor_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']

        for col in sensor_cols:
            signal = window[col].values

            # 时域特征 (17维)
            time_features = extract_time_features(signal)
            all_features.extend(time_features)

            # 频域特征 (10维)
            freq_features = extract_frequency_features(signal, self.sampling_rate)
            all_features.extend(freq_features)

        # 合成特征 (11维)
        composite_features = extract_composite_features(window)
        all_features.extend(composite_features)

        return all_features

# 总特征数: 6轴 × (17时域 + 10频域) + 11合成 = 173维
```

---

## 3. 模型选择

### 3.1 算法对比

| 算法 | 优点 | 缺点 | 训练时间 | 准确率 | 推荐度 |
|-----|------|------|---------|--------|-------|
| **Random Forest** | 易用、快速、可解释性好 | 模型较大 | 快 | 90-92% | ⭐⭐⭐⭐⭐ |
| **XGBoost** | 准确率高、防过拟合 | 调参复杂 | 中等 | 92-95% | ⭐⭐⭐⭐ |
| **SVM** | 小样本效果好 | 大数据集慢 | 慢 | 88-90% | ⭐⭐⭐ |
| **Logistic Regression** | 简单、快速 | 准确率低 | 快 | 85-87% | ⭐⭐ |
| **CNN-LSTM** | 准确率最高 | 需要大量数据 | 很慢 | 95-98% | ⭐⭐⭐⭐ |

### 3.2 Random Forest（推荐）

```python
from sklearn.ensemble import RandomForestClassifier

# 配置
rf_config = {
    'n_estimators': 100,        # 树的数量
    'max_depth': 20,            # 最大深度
    'min_samples_split': 5,     # 分裂最小样本数
    'min_samples_leaf': 2,      # 叶节点最小样本数
    'max_features': 'sqrt',     # 特征采样数量
    'bootstrap': True,          # 是否Bootstrap采样
    'random_state': 42,         # 随机种子
    'n_jobs': -1               # 并行数（-1表示全部CPU）
}

model = RandomForestClassifier(**rf_config)
```

### 3.3 XGBoost（备选）

```python
import xgboost as xgb

# 配置
xgb_config = {
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42
}

model = xgb.XGBClassifier(**xgb_config)
```

---

## 4. 训练流程

### 4.1 完整训练脚本

```python
# training/train_model.py
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import json
from datetime import datetime

class ModelTrainer:
    """模型训练器"""

    def __init__(self, db_url):
        self.db_url = db_url
        self.data_loader = DataLoader(db_url)
        self.feature_extractor = FeatureExtractor(window_size=100, overlap=50)
        self.model = None

    def prepare_data(self):
        """准备训练数据"""
        print("📊 加载数据...")

        # 1. 加载原始数据
        data_list, labels = self.data_loader.load_all_data()

        print(f"✅ 加载了 {len(data_list)} 个样本")

        # 2. 数据清洗
        data_list = [clean_data(df) for df in data_list]

        # 3. 特征提取
        print("🔧 提取特征...")
        X_list = []
        y_list = []

        for data, label in zip(data_list, labels):
            features = self.feature_extractor.extract_features(data)

            for feature_vec in features:
                X_list.append(feature_vec)
                y_list.append(label)

        X = np.array(X_list)
        y = np.array(y_list)

        print(f"✅ 提取了 {len(X)} 个特征向量 (维度: {X.shape[1]})")

        return X, y

    def train(self, X, y, test_size=0.2, val_size=0.1):
        """训练模型"""
        print("\n🚀 开始训练...")

        # 1. 划分数据集
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size/(1-test_size),
            random_state=42, stratify=y_temp
        )

        print(f"📊 数据集划分:")
        print(f"   训练集: {len(X_train)} 样本")
        print(f"   验证集: {len(X_val)} 样本")
        print(f"   测试集: {len(X_test)} 样本")

        # 2. 训练模型
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            random_state=42,
            n_jobs=-1,
            verbose=1
        )

        self.model.fit(X_train, y_train)
        print("✅ 训练完成")

        # 3. 验证集评估
        val_score = self.model.score(X_val, y_val)
        print(f"📈 验证集准确率: {val_score:.3f}")

        # 4. 测试集评估
        metrics = self.evaluate(X_test, y_test)

        return metrics

    def evaluate(self, X_test, y_test):
        """评估模型"""
        print("\n📊 模型评估:")

        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]

        # 分类报告
        print(classification_report(y_test, y_pred,
                                   target_names=['Normal', 'Fall']))

        # 混淆矩阵
        cm = confusion_matrix(y_test, y_pred)
        print("\n混淆矩阵:")
        print(cm)

        # 计算指标
        tn, fp, fn, tp = cm.ravel()
        metrics = {
            'accuracy': (tp + tn) / (tp + tn + fp + fn),
            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'f1_score': 2*tp / (2*tp + fp + fn) if (2*tp + fp + fn) > 0 else 0,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn)
        }

        print(f"\n📈 综合指标:")
        print(f"   准确率: {metrics['accuracy']:.3f}")
        print(f"   精确率: {metrics['precision']:.3f}")
        print(f"   召回率: {metrics['recall']:.3f}")
        print(f"   F1分数: {metrics['f1_score']:.3f}")

        return metrics

    def save_model(self, output_dir='./models'):
        """保存模型"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"{output_dir}/fall_detection_rf_{timestamp}.pkl"

        joblib.dump(self.model, model_path)
        print(f"💾 模型已保存: {model_path}")

        return model_path

# 使用示例
if __name__ == "__main__":
    trainer = ModelTrainer(
        db_url='mysql+pymysql://root:password@localhost/fall_detection_training'
    )

    # 准备数据
    X, y = trainer.prepare_data()

    # 训练模型
    metrics = trainer.train(X, y)

    # 保存模型
    model_path = trainer.save_model()

    print("\n✅ 训练完成！")
```

---

## 5. 模型评估

### 5.1 交叉验证

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

def cross_validation(model, X, y, cv=5):
    """K折交叉验证"""

    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)

    scores = cross_val_score(model, X, y, cv=skf, scoring='f1')

    print(f"交叉验证 F1分数: {scores}")
    print(f"平均: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")

    return scores
```

### 5.2 ROC曲线

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

def plot_roc_curve(y_test, y_proba):
    """绘制ROC曲线"""

    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')
    plt.show()

    return roc_auc
```

### 5.3 特征重要性

```python
def plot_feature_importance(model, feature_names, top_n=20):
    """绘制特征重要性"""

    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    plt.figure(figsize=(10, 6))
    plt.title(f'Top {top_n} Feature Importances')
    plt.barh(range(top_n), importances[indices])
    plt.yticks(range(top_n), [feature_names[i] for i in indices])
    plt.xlabel('Importance')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
```

---

## 6. 超参数调优

### 6.1 网格搜索

```python
from sklearn.model_selection import GridSearchCV

def grid_search_tuning(X_train, y_train):
    """网格搜索调优"""

    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    rf = RandomForestClassifier(random_state=42, n_jobs=-1)

    grid_search = GridSearchCV(
        rf, param_grid, cv=5,
        scoring='f1', verbose=2, n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    print(f"最佳参数: {grid_search.best_params_}")
    print(f"最佳F1分数: {grid_search.best_score_:.3f}")

    return grid_search.best_estimator_
```

---

## 7. 数据增强

### 7.1 时间序列增强

```python
def augment_data(sensor_data, label, n_augmentations=5):
    """数据增强"""

    augmented_data = [sensor_data]  # 原始数据

    for _ in range(n_augmentations):
        # 1. 添加噪声
        noise = np.random.normal(0, 0.1, sensor_data.shape)
        noisy_data = sensor_data + noise

        # 2. 缩放
        scale_factor = np.random.uniform(0.9, 1.1)
        scaled_data = sensor_data * scale_factor

        # 3. 时间偏移
        shift = np.random.randint(-5, 5)
        shifted_data = np.roll(sensor_data, shift, axis=0)

        augmented_data.extend([noisy_data, scaled_data, shifted_data])

    return augmented_data
```

---

## 总结

本文档详细介绍了模型训练的完整流程。关键要点：

1. ✅ **数据准备**: 加载、清洗、统计分析
2. ✅ **特征工程**: 时域、频域、合成特征（173维）
3. ✅ **模型选择**: Random Forest（推荐）
4. ✅ **训练流程**: 完整的训练脚本
5. ✅ **模型评估**: 多种评估方法
6. ✅ **超参数调优**: 网格搜索
7. ✅ **数据增强**: 提高模型泛化能力

**下一步:** 阅读 `06-模型部署.md` 了解如何将模型部署到小程序。
