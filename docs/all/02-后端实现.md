# CycloSafe 后端实现详解

本文档详细介绍 CycloSafe Python 后端的技术实现，包括 FastAPI 应用、MySQL 数据库、机器学习训练系统和模型管理。

---

## 目录

- [技术栈](#技术栈)
- [项目结构](#项目结构)
- [数据库设计](#数据库设计)
- [API 接口](#api-接口)
- [机器学习训练](#机器学习训练)
- [模型管理](#模型管理)
- [安全性](#安全性)
- [性能优化](#性能优化)

---

## 技术栈

### 核心框架

| 技术 | 版本 | 用途 |
|------|------|------|
| FastAPI | latest | Web 框架 |
| Uvicorn | latest | ASGI 服务器 |
| SQLAlchemy | latest | ORM |
| MySQL | 8.0+ | 关系型数据库 |
| PyMySQL | latest | MySQL 驱动 |

### 机器学习

| 库名 | 用途 |
|------|------|
| scikit-learn | 分类器（随机森林、SVM） |
| pandas | 数据处理 |
| numpy | 数值计算 |
| scipy | 信号处理（FFT、滤波） |
| joblib | 模型序列化 |

### 其他依赖

- **python-dotenv** - 环境变量管理
- **pydantic** - 数据验证
- **logging** - 日志系统

---

## 项目结构

```
backend/
├── main.py                   # FastAPI 应用入口（462行）
├── config.py                 # 配置文件
├── models.py                 # SQLAlchemy 模型
├── database.py               # 数据库连接
├── requirements.txt          # Python 依赖
├── .env                      # 环境变量（gitignored）
├── init_database.sql         # 数据库初始化 SQL
├── start.sh                  # 启动脚本
│
├── train_model.py            # 模型训练主脚本（600+行）
├── export_model.py           # 模型导出工具
├── test_data_upload.py       # 测试脚本
│
├── training/                 # 训练模块
│   ├── feature_extraction.py # 特征提取
│   └── train_model.py        # 训练逻辑
│
├── models/                   # 模型存储目录
│   └── fall_detection_*.pkl  # 训练好的模型
│
├── logs/                     # 日志存储
│   └── training_*.log        # 训练日志
│
└── venv/                     # Python 虚拟环境
```

---

## 数据库设计

### 数据库架构

**数据库名称**：`fall_detection_training`

#### 表结构

```
fall_detection_training
├── training_samples          # 训练样本主表
├── sensor_data               # 传感器数据表（大表）
├── training_jobs             # 训练任务表
└── model_versions            # 模型版本表
```

---

### 1. training_samples（训练样本表）

存储上传的训练样本元数据。

```sql
CREATE TABLE training_samples (
  id                INT AUTO_INCREMENT PRIMARY KEY,
  user_id           VARCHAR(64) NOT NULL,
  label             ENUM('fall', 'normal') NOT NULL,
  source            ENUM('simulate', 'riding') NOT NULL,
  duration          INT NOT NULL,                    -- 持续时间（毫秒）
  sample_count      INT NOT NULL,                    -- 样本数量
  uploaded_at       DATETIME DEFAULT CURRENT_TIMESTAMP,
  processed         BOOLEAN DEFAULT FALSE,           -- 是否已处理用于训练
  sample_metadata   JSON,                            -- 元数据（设备信息等）

  INDEX idx_label (label),
  INDEX idx_source (source),
  INDEX idx_uploaded (uploaded_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

**字段说明**：
- `user_id`：用户标识（可匿名）
- `label`：标签（'fall' 摔倒 / 'normal' 正常）
- `source`：数据来源（'simulate' 模拟 / 'riding' 真实骑行）
- `duration`：采样时长（毫秒）
- `sample_count`：传感器数据点数量
- `processed`：是否已用于训练
- `sample_metadata`：JSON格式元数据

**SQLAlchemy 模型**：

```python
from sqlalchemy import Column, Integer, String, DateTime, Boolean, JSON, Enum
from sqlalchemy.sql import func
from database import Base

class TrainingSample(Base):
    __tablename__ = 'training_samples'

    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(String(64), nullable=False)
    label = Column(Enum('fall', 'normal'), nullable=False)
    source = Column(Enum('simulate', 'riding'), nullable=False)
    duration = Column(Integer, nullable=False)
    sample_count = Column(Integer, nullable=False)
    uploaded_at = Column(DateTime, default=func.now())
    processed = Column(Boolean, default=False)
    sample_metadata = Column(JSON, nullable=True)
```

---

### 2. sensor_data（传感器数据表）

存储原始传感器数据（加速度计、陀螺仪）。

```sql
CREATE TABLE sensor_data (
  id                BIGINT AUTO_INCREMENT PRIMARY KEY,
  sample_id         INT NOT NULL,
  timestamp         BIGINT NOT NULL,                 -- 时间戳（毫秒）
  acc_x             FLOAT NOT NULL,                  -- 加速度 X 轴（m/s²）
  acc_y             FLOAT NOT NULL,                  -- 加速度 Y 轴
  acc_z             FLOAT NOT NULL,                  -- 加速度 Z 轴
  gyro_x            FLOAT NOT NULL,                  -- 陀螺仪 X 轴（°/s）
  gyro_y            FLOAT NOT NULL,                  -- 陀螺仪 Y 轴
  gyro_z            FLOAT NOT NULL,                  -- 陀螺仪 Z 轴

  FOREIGN KEY (sample_id) REFERENCES training_samples(id) ON DELETE CASCADE,
  INDEX idx_sample_id (sample_id),
  INDEX idx_timestamp (timestamp)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

**字段说明**：
- `sample_id`：关联 training_samples 表
- `timestamp`：采样时间戳
- `acc_x/y/z`：三轴加速度
- `gyro_x/y/z`：三轴角速度

**SQLAlchemy 模型**：

```python
from sqlalchemy import Column, BigInteger, Integer, Float, ForeignKey

class SensorData(Base):
    __tablename__ = 'sensor_data'

    id = Column(BigInteger, primary_key=True, autoincrement=True)
    sample_id = Column(Integer, ForeignKey('training_samples.id', ondelete='CASCADE'), nullable=False)
    timestamp = Column(BigInteger, nullable=False)
    acc_x = Column(Float, nullable=False)
    acc_y = Column(Float, nullable=False)
    acc_z = Column(Float, nullable=False)
    gyro_x = Column(Float, nullable=False)
    gyro_y = Column(Float, nullable=False)
    gyro_z = Column(Float, nullable=False)
```

---

### 3. training_jobs（训练任务表）

记录模型训练任务及其结果。

```sql
CREATE TABLE training_jobs (
  id                INT AUTO_INCREMENT PRIMARY KEY,
  status            ENUM('pending', 'running', 'completed', 'failed') DEFAULT 'pending',
  algorithm         VARCHAR(64) NOT NULL,            -- 算法（random_forest/svm）
  train_samples     INT,                             -- 训练样本数
  val_samples       INT,                             -- 验证样本数
  test_samples      INT,                             -- 测试样本数
  accuracy          FLOAT,                           -- 准确率
  precision_val     FLOAT,                           -- 精确率
  recall_val        FLOAT,                           -- 召回率
  f1_score          FLOAT,                           -- F1 分数
  auc_roc           FLOAT,                           -- AUC-ROC
  true_positives    INT,                             -- 真阳性
  true_negatives    INT,                             -- 真阴性
  false_positives   INT,                             -- 假阳性
  false_negatives   INT,                             -- 假阴性
  model_path        VARCHAR(255),                    -- 模型文件路径
  tfjs_model_path   VARCHAR(255),                    -- TensorFlow.js 模型路径
  config            JSON,                            -- 训练配置
  error_message     TEXT,                            -- 错误信息
  log_file          VARCHAR(255),                    -- 日志文件路径
  started_at        DATETIME,
  completed_at      DATETIME,
  created_at        DATETIME DEFAULT CURRENT_TIMESTAMP,

  INDEX idx_status (status),
  INDEX idx_created (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

**SQLAlchemy 模型**：

```python
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, Enum, JSON

class TrainingJob(Base):
    __tablename__ = 'training_jobs'

    id = Column(Integer, primary_key=True, autoincrement=True)
    status = Column(Enum('pending', 'running', 'completed', 'failed'), default='pending')
    algorithm = Column(String(64), nullable=False)
    train_samples = Column(Integer, nullable=True)
    val_samples = Column(Integer, nullable=True)
    test_samples = Column(Integer, nullable=True)
    accuracy = Column(Float, nullable=True)
    precision_val = Column(Float, nullable=True)
    recall_val = Column(Float, nullable=True)
    f1_score = Column(Float, nullable=True)
    auc_roc = Column(Float, nullable=True)
    true_positives = Column(Integer, nullable=True)
    true_negatives = Column(Integer, nullable=True)
    false_positives = Column(Integer, nullable=True)
    false_negatives = Column(Integer, nullable=True)
    model_path = Column(String(255), nullable=True)
    tfjs_model_path = Column(String(255), nullable=True)
    config = Column(JSON, nullable=True)
    error_message = Column(Text, nullable=True)
    log_file = Column(String(255), nullable=True)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=func.now())
```

---

### 4. model_versions（模型版本表）

管理部署的模型版本。

```sql
CREATE TABLE model_versions (
  id                INT AUTO_INCREMENT PRIMARY KEY,
  version           VARCHAR(32) NOT NULL UNIQUE,     -- 版本号（如 "1.0.0"）
  job_id            INT NOT NULL,                    -- 关联训练任务
  algorithm         VARCHAR(64) NOT NULL,            -- 算法
  model_url         VARCHAR(512) NOT NULL,           -- 模型下载URL
  model_size_mb     FLOAT NOT NULL,                  -- 模型大小（MB）
  checksum          VARCHAR(64) NOT NULL,            -- 校验和（SHA-256）
  accuracy          FLOAT NOT NULL,                  -- 准确率
  f1_score          FLOAT NOT NULL,                  -- F1 分数
  is_active         BOOLEAN DEFAULT TRUE,            -- 是否激活
  download_count    INT DEFAULT 0,                   -- 下载次数
  deployed_at       DATETIME DEFAULT CURRENT_TIMESTAMP,

  FOREIGN KEY (job_id) REFERENCES training_jobs(id),
  INDEX idx_version (version),
  INDEX idx_active (is_active),
  INDEX idx_deployed (deployed_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

**SQLAlchemy 模型**：

```python
class ModelVersion(Base):
    __tablename__ = 'model_versions'

    id = Column(Integer, primary_key=True, autoincrement=True)
    version = Column(String(32), nullable=False, unique=True)
    job_id = Column(Integer, ForeignKey('training_jobs.id'), nullable=False)
    algorithm = Column(String(64), nullable=False)
    model_url = Column(String(512), nullable=False)
    model_size_mb = Column(Float, nullable=False)
    checksum = Column(String(64), nullable=False)
    accuracy = Column(Float, nullable=False)
    f1_score = Column(Float, nullable=False)
    is_active = Column(Boolean, default=True)
    download_count = Column(Integer, default=0)
    deployed_at = Column(DateTime, default=func.now())
```

---

### 数据库初始化

**init_database.sql**：

```sql
-- 创建数据库
CREATE DATABASE IF NOT EXISTS fall_detection_training
  DEFAULT CHARACTER SET utf8mb4
  DEFAULT COLLATE utf8mb4_unicode_ci;

USE fall_detection_training;

-- 创建表（SQL 略，见上方）
```

**在 database.py 中自动创建**：

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from config import DATABASE_URL

# 创建引擎
engine = create_engine(DATABASE_URL, echo=False)

# 创建会话
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# 基类
Base = declarative_base()

# 依赖注入
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# 创建所有表
def init_db():
    import models  # 导入所有模型
    Base.metadata.create_all(bind=engine)
```

---

## API 接口

### FastAPI 应用架构

```python
from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="Fall Detection Training API",
    version="1.0.0",
    description="摔倒检测机器学习训练系统API"
)

# CORS 中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

### 接口分类

#### 1. 公开接口（无需认证）

| 接口 | 方法 | 描述 |
|------|------|------|
| `/` | GET | 服务状态 |
| `/api/health` | GET | 健康检查 |
| `/api/upload/training-data` | POST | 上传训练数据 |
| `/api/model/download` | GET | 下载最新模型 |
| `/api/model/version` | GET | 获取模型版本 |

#### 2. 管理接口（需要 API Key）

| 接口 | 方法 | 描述 |
|------|------|------|
| `/api/data/stats` | GET | 数据统计 |
| `/api/data/list` | GET | 数据列表 |
| `/api/training/start` | POST | 启动训练 |
| `/api/training/status/{job_id}` | GET | 训练状态 |
| `/api/model/upload` | POST | 上传模型版本 |
| `/api/model/versions` | GET | 模型版本列表 |

---

### API 认证

**API Key 验证**：

```python
from fastapi import Header, HTTPException
from config import ADMIN_API_KEY

def verify_api_key(x_api_key: str = Header(...)):
    """验证 API Key"""
    if x_api_key != ADMIN_API_KEY:
        raise HTTPException(
            status_code=401,
            detail="Unauthorized: Invalid API Key",
            headers={"WWW-Authenticate": "X-API-Key"}
        )
    return x_api_key
```

**使用方式**：

```python
@app.get("/api/data/stats")
def get_data_stats(
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)  # 需要 API Key
):
    # 接口逻辑
    pass
```

---

### 核心接口实现

#### 1. 上传训练数据

**接口**：`POST /api/upload/training-data`

**请求体**：

```python
from pydantic import BaseModel
from typing import List, Optional

class SensorDataPoint(BaseModel):
    timestamp: int
    acc: dict  # {x, y, z}
    gyro: dict  # {x, y, z}

class UploadDataRequest(BaseModel):
    user_id: str
    label: str  # 'fall' or 'normal'
    source: str  # 'simulate' or 'riding'
    duration: int
    samples: List[SensorDataPoint]
    metadata: Optional[dict] = None
```

**实现**：

```python
@app.post("/api/upload/training-data")
async def upload_training_data(
    request: UploadDataRequest,
    db: Session = Depends(get_db)
):
    try:
        # 1. 验证数据
        if request.label not in ['fall', 'normal']:
            raise HTTPException(status_code=400, detail="label must be 'fall' or 'normal'")

        if request.source not in ['simulate', 'riding']:
            raise HTTPException(status_code=400, detail="source must be 'simulate' or 'riding'")

        if len(request.samples) == 0:
            raise HTTPException(status_code=400, detail="samples cannot be empty")

        # 2. 创建样本记录
        sample = TrainingSample(
            user_id=request.user_id,
            label=request.label,
            source=request.source,
            duration=request.duration,
            sample_count=len(request.samples),
            sample_metadata=request.metadata
        )
        db.add(sample)
        db.commit()
        db.refresh(sample)

        # 3. 批量插入传感器数据
        sensor_data_list = []
        for point in request.samples:
            sensor_data_list.append(SensorData(
                sample_id=sample.id,
                timestamp=point.timestamp,
                acc_x=point.acc['x'],
                acc_y=point.acc['y'],
                acc_z=point.acc['z'],
                gyro_x=point.gyro['x'],
                gyro_y=point.gyro['y'],
                gyro_z=point.gyro['z']
            ))

        db.bulk_save_objects(sensor_data_list)
        db.commit()

        logger.info(f"数据保存成功: sample_id={sample.id}")

        return {
            "success": True,
            "message": "数据上传成功",
            "data": {
                "sample_id": sample.id,
                "sample_count": len(request.samples),
                "created_at": sample.uploaded_at.isoformat()
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"上传失败: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
```

**响应示例**：

```json
{
  "success": true,
  "message": "数据上传成功",
  "data": {
    "sample_id": 123,
    "sample_count": 500,
    "created_at": "2025-10-04T12:00:00"
  }
}
```

---

#### 2. 获取数据统计

**接口**：`GET /api/data/stats`（需要 API Key）

**实现**：

```python
@app.get("/api/data/stats")
def get_data_stats(
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)
):
    try:
        total = db.query(TrainingSample).count()
        fall_count = db.query(TrainingSample).filter(TrainingSample.label == 'fall').count()
        normal_count = db.query(TrainingSample).filter(TrainingSample.label == 'normal').count()

        # 按来源统计
        simulate_count = db.query(TrainingSample).filter(TrainingSample.source == 'simulate').count()
        riding_count = db.query(TrainingSample).filter(TrainingSample.source == 'riding').count()

        return {
            "success": True,
            "data": {
                "total_samples": total,
                "fall_samples": fall_count,
                "normal_samples": normal_count,
                "by_source": {
                    "simulate": simulate_count,
                    "riding": riding_count
                }
            }
        }
    except Exception as e:
        logger.error(f"查询统计失败: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

**响应示例**：

```json
{
  "success": true,
  "data": {
    "total_samples": 500,
    "fall_samples": 100,
    "normal_samples": 400,
    "by_source": {
      "simulate": 50,
      "riding": 450
    }
  }
}
```

---

#### 3. 启动训练任务

**接口**：`POST /api/training/start`（需要 API Key）

**实现**：

```python
@app.post("/api/training/start")
async def start_training(
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)
):
    try:
        # 检查是否有正在运行的任务
        running_job = db.query(TrainingJob).filter(
            TrainingJob.status.in_(['pending', 'running'])
        ).first()

        if running_job:
            raise HTTPException(status_code=409, detail="已有训练任务正在运行")

        # 创建新任务
        job = TrainingJob(
            algorithm='random_forest',
            status='pending'
        )
        db.add(job)
        db.commit()
        db.refresh(job)

        # TODO: 启动异步训练任务（使用 asyncio 或 Celery）
        # asyncio.create_task(run_training(job.id))

        logger.info(f"创建训练任务: job_id={job.id}")

        return {
            "success": True,
            "message": "训练任务已创建",
            "data": {
                "job_id": job.id,
                "status": job.status
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"创建训练任务失败: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
```

---

#### 4. 获取模型版本

**接口**：`GET /api/model/version`（公开接口）

**实现**：

```python
@app.get("/api/model/version")
def get_model_version(db: Session = Depends(get_db)):
    try:
        # 查询最新的活跃模型版本
        latest_model = db.query(ModelVersion).filter(
            ModelVersion.is_active == True
        ).order_by(ModelVersion.deployed_at.desc()).first()

        if not latest_model:
            raise HTTPException(status_code=404, detail="暂无可用模型")

        logger.info(f"版本检查: version={latest_model.version}")

        return {
            "success": True,
            "data": {
                "version": latest_model.version,
                "checksum": latest_model.checksum,
                "model_url": latest_model.model_url,
                "model_size_mb": latest_model.model_size_mb,
                "deployed_at": latest_model.deployed_at.isoformat(),
                "algorithm": latest_model.algorithm,
                "accuracy": latest_model.accuracy,
                "f1_score": latest_model.f1_score
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"获取模型版本失败: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

#### 5. 下载模型

**接口**：`GET /api/model/download`（公开接口）

**实现**：

```python
@app.get("/api/model/download")
def download_model(db: Session = Depends(get_db)):
    try:
        # 查询最新的活跃模型版本
        latest_model = db.query(ModelVersion).filter(
            ModelVersion.is_active == True
        ).order_by(ModelVersion.deployed_at.desc()).first()

        if not latest_model:
            raise HTTPException(status_code=404, detail="暂无可用模型")

        # 增加下载计数
        latest_model.download_count += 1
        db.commit()

        logger.info(f"模型下载: version={latest_model.version}, total_downloads={latest_model.download_count}")

        return {
            "success": True,
            "data": {
                "version": latest_model.version,
                "algorithm": latest_model.algorithm,
                "model_url": latest_model.model_url,
                "model_size_mb": latest_model.model_size_mb,
                "checksum": latest_model.checksum,
                "accuracy": latest_model.accuracy,
                "f1_score": latest_model.f1_score,
                "deployed_at": latest_model.deployed_at.isoformat(),
                "download_count": latest_model.download_count
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"获取模型失败: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## 机器学习训练

### 训练流程

```
1. 数据加载
   ↓
2. 数据清洗（异常值检测）
   ↓
3. 特征提取（时域 + 频域）
   ↓
4. 数据划分（训练集/验证集/测试集）
   ↓
5. 模型训练（随机森林 / SVM）
   ↓
6. 交叉验证
   ↓
7. 超参数调优
   ↓
8. 模型评估
   ↓
9. 模型保存
```

---

### 1. 数据加载

**从数据库加载**：

```python
import pandas as pd
from sqlalchemy import create_engine
from config import DATABASE_URL

def load_training_data():
    """从数据库加载训练数据"""
    engine = create_engine(DATABASE_URL)

    # 查询样本
    samples_query = """
    SELECT id, label, source, sample_count
    FROM training_samples
    WHERE processed = FALSE
    """
    samples_df = pd.read_sql(samples_query, engine)

    all_data = []

    for _, sample in samples_df.iterrows():
        # 查询传感器数据
        sensor_query = f"""
        SELECT timestamp, acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z
        FROM sensor_data
        WHERE sample_id = {sample['id']}
        ORDER BY timestamp
        """
        sensor_df = pd.read_sql(sensor_query, engine)

        # 添加标签
        sensor_df['label'] = sample['label']
        sensor_df['sample_id'] = sample['id']

        all_data.append(sensor_df)

    # 合并所有数据
    data = pd.concat(all_data, ignore_index=True)

    logger.info(f"加载数据: {len(data)} 个数据点, {len(samples_df)} 个样本")

    return data, samples_df
```

---

### 2. 数据清洗

**DataCleaner 类**：

```python
from scipy import stats
import numpy as np

class DataCleaner:
    """数据清洗器"""

    def __init__(self, threshold_std=5.0):
        self.threshold_std = threshold_std

    def remove_outliers_zscore(self, data, axis_names=['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']):
        """
        使用 Z-score 方法移除异常值
        """
        logger.info(f"数据清洗前: {len(data)} 个数据点")

        # 计算 Z-score
        z_scores = np.abs(stats.zscore(data[axis_names], nan_policy='omit'))

        # 移除任意轴超过阈值的数据点
        mask = (z_scores < self.threshold_std).all(axis=1)
        cleaned_data = data[mask].copy()

        removed_count = len(data) - len(cleaned_data)
        logger.info(f"数据清洗后: {len(cleaned_data)} 个数据点 (移除 {removed_count} 个异常点)")

        return cleaned_data

    def remove_outliers_iqr(self, data, axis_names=['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']):
        """
        使用 IQR（四分位距）方法移除异常值
        """
        logger.info(f"使用 IQR 方法清洗数据，原始数据: {len(data)} 个数据点")

        cleaned_data = data.copy()

        for col in axis_names:
            Q1 = cleaned_data[col].quantile(0.25)
            Q3 = cleaned_data[col].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # 移除超出范围的数据
            cleaned_data = cleaned_data[
                (cleaned_data[col] >= lower_bound) &
                (cleaned_data[col] <= upper_bound)
            ]

        removed_count = len(data) - len(cleaned_data)
        logger.info(f"IQR 清洗后: {len(cleaned_data)} 个数据点 (移除 {removed_count} 个异常点)")

        return cleaned_data
```

---

### 3. 特征提取

**FeatureExtractor 类**：

```python
from scipy.fft import fft
from scipy.signal import find_peaks

class FeatureExtractor:
    """特征提取器"""

    def extract_features(self, sensor_data):
        """
        从传感器数据中提取特征
        返回：特征向量（30+ 维）
        """
        features = {}

        # 1. 时域特征（加速度）
        features['acc_mean'] = sensor_data[['acc_x', 'acc_y', 'acc_z']].mean().mean()
        features['acc_std'] = sensor_data[['acc_x', 'acc_y', 'acc_z']].std().mean()
        features['acc_max'] = sensor_data[['acc_x', 'acc_y', 'acc_z']].max().max()
        features['acc_min'] = sensor_data[['acc_x', 'acc_y', 'acc_z']].min().min()
        features['acc_range'] = features['acc_max'] - features['acc_min']

        # 加速度模
        acc_mag = np.sqrt(
            sensor_data['acc_x']**2 +
            sensor_data['acc_y']**2 +
            sensor_data['acc_z']**2
        )
        features['acc_mag_mean'] = acc_mag.mean()
        features['acc_mag_std'] = acc_mag.std()
        features['acc_mag_max'] = acc_mag.max()

        # 2. 时域特征（陀螺仪）
        features['gyro_mean'] = sensor_data[['gyro_x', 'gyro_y', 'gyro_z']].mean().mean()
        features['gyro_std'] = sensor_data[['gyro_x', 'gyro_y', 'gyro_z']].std().mean()
        features['gyro_max'] = sensor_data[['gyro_x', 'gyro_y', 'gyro_z']].max().max()
        features['gyro_min'] = sensor_data[['gyro_x', 'gyro_y', 'gyro_z']].min().min()

        # 陀螺仪模
        gyro_mag = np.sqrt(
            sensor_data['gyro_x']**2 +
            sensor_data['gyro_y']**2 +
            sensor_data['gyro_z']**2
        )
        features['gyro_mag_mean'] = gyro_mag.mean()
        features['gyro_mag_std'] = gyro_mag.std()
        features['gyro_mag_max'] = gyro_mag.max()

        # 3. 频域特征（FFT）
        # 加速度 FFT
        acc_fft = np.abs(fft(acc_mag))
        features['acc_fft_mean'] = acc_fft.mean()
        features['acc_fft_std'] = acc_fft.std()
        features['acc_fft_max'] = acc_fft.max()

        # 主频率（能量最大的频率）
        fft_energy = acc_fft**2
        features['acc_dominant_freq'] = np.argmax(fft_energy)

        # 陀螺仪 FFT
        gyro_fft = np.abs(fft(gyro_mag))
        features['gyro_fft_mean'] = gyro_fft.mean()
        features['gyro_fft_std'] = gyro_fft.std()
        features['gyro_fft_max'] = gyro_fft.max()

        # 4. 峰值特征
        acc_peaks, _ = find_peaks(acc_mag, height=10)
        features['acc_peak_count'] = len(acc_peaks)

        gyro_peaks, _ = find_peaks(gyro_mag, height=50)
        features['gyro_peak_count'] = len(gyro_peaks)

        # 5. 变化率特征
        acc_diff = np.diff(acc_mag)
        features['acc_change_mean'] = acc_diff.mean()
        features['acc_change_std'] = acc_diff.std()

        gyro_diff = np.diff(gyro_mag)
        features['gyro_change_mean'] = gyro_diff.mean()
        features['gyro_change_std'] = gyro_diff.std()

        return features

    def extract_features_from_samples(self, data):
        """
        从所有样本中提取特征
        返回：特征矩阵 DataFrame
        """
        feature_list = []

        for sample_id, group in data.groupby('sample_id'):
            features = self.extract_features(group)
            features['label'] = group['label'].iloc[0]
            features['sample_id'] = sample_id
            feature_list.append(features)

        features_df = pd.DataFrame(feature_list)

        logger.info(f"特征提取完成: {len(features_df)} 个样本, {len(features_df.columns) - 2} 个特征")

        return features_df
```

---

### 4. 模型训练

**ModelTrainer 类**：

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import joblib

class ModelTrainer:
    """模型训练器"""

    def __init__(self, algorithm='random_forest'):
        self.algorithm = algorithm
        self.model = None

    def train(self, X_train, y_train):
        """训练模型"""
        logger.info(f"开始训练模型: {self.algorithm}")

        if self.algorithm == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
        elif self.algorithm == 'svm':
            self.model = SVC(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                random_state=42
            )
        else:
            raise ValueError(f"不支持的算法: {self.algorithm}")

        self.model.fit(X_train, y_train)

        logger.info("模型训练完成")

    def evaluate(self, X_test, y_test):
        """评估模型"""
        y_pred = self.model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, pos_label='fall')
        recall = recall_score(y_test, y_pred, pos_label='fall')
        f1 = f1_score(y_test, y_pred, pos_label='fall')

        # 混淆矩阵
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=['normal', 'fall']).ravel()

        logger.info(f"模型评估结果:")
        logger.info(f"  准确率: {accuracy:.4f}")
        logger.info(f"  精确率: {precision:.4f}")
        logger.info(f"  召回率: {recall:.4f}")
        logger.info(f"  F1 分数: {f1:.4f}")
        logger.info(f"  混淆矩阵: TN={tn}, FP={fp}, FN={fn}, TP={tp}")

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        }

    def cross_validate(self, X, y, cv=5):
        """交叉验证"""
        logger.info(f"开始 {cv} 折交叉验证...")

        scores = cross_val_score(self.model, X, y, cv=cv, scoring='f1')

        logger.info(f"交叉验证 F1 分数: {scores}")
        logger.info(f"平均 F1 分数: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

        return scores

    def hyperparameter_tuning(self, X_train, y_train):
        """超参数调优"""
        logger.info("开始超参数调优...")

        if self.algorithm == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15, None],
                'min_samples_split': [2, 5, 10]
            }
            base_model = RandomForestClassifier(random_state=42, n_jobs=-1)

        elif self.algorithm == 'svm':
            param_grid = {
                'C': [0.1, 1, 10],
                'gamma': ['scale', 'auto', 0.1, 1],
                'kernel': ['rbf', 'linear']
            }
            base_model = SVC(random_state=42)

        else:
            raise ValueError(f"不支持的算法: {self.algorithm}")

        grid_search = GridSearchCV(
            base_model,
            param_grid,
            cv=5,
            scoring='f1',
            n_jobs=-1,
            verbose=1
        )

        grid_search.fit(X_train, y_train)

        logger.info(f"最佳参数: {grid_search.best_params_}")
        logger.info(f"最佳 F1 分数: {grid_search.best_score_:.4f}")

        self.model = grid_search.best_estimator_

        return grid_search.best_params_, grid_search.best_score_

    def save(self, filepath):
        """保存模型"""
        joblib.dump(self.model, filepath)
        logger.info(f"模型已保存: {filepath}")

    def load(self, filepath):
        """加载模型"""
        self.model = joblib.load(filepath)
        logger.info(f"模型已加载: {filepath}")
```

---

### 5. 完整训练流程

**train_model.py 主函数**：

```python
import os
from datetime import datetime
from database import get_db
from models import TrainingJob

def run_training(job_id):
    """运行训练任务"""
    db = next(get_db())

    try:
        # 1. 更新任务状态
        job = db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
        job.status = 'running'
        job.started_at = datetime.now()
        db.commit()

        logger.info(f"[Job {job_id}] 开始训练...")

        # 2. 加载数据
        data, samples_df = load_training_data()

        # 3. 数据清洗
        cleaner = DataCleaner(threshold_std=5.0)
        cleaned_data = cleaner.remove_outliers_zscore(data)

        # 4. 特征提取
        extractor = FeatureExtractor()
        features_df = extractor.extract_features_from_samples(cleaned_data)

        # 5. 数据划分
        X = features_df.drop(['label', 'sample_id'], axis=1)
        y = features_df['label']

        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )

        logger.info(f"数据划分: 训练集 {len(X_train)}, 验证集 {len(X_val)}, 测试集 {len(X_test)}")

        # 6. 模型训练
        trainer = ModelTrainer(algorithm='random_forest')

        # 超参数调优
        best_params, best_score = trainer.hyperparameter_tuning(X_train, y_train)

        # 7. 模型评估
        metrics = trainer.evaluate(X_test, y_test)

        # 8. 交叉验证
        cv_scores = trainer.cross_validate(X_train, y_train, cv=5)

        # 9. 保存模型
        model_filename = f"fall_detection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        model_path = os.path.join('models', model_filename)
        trainer.save(model_path)

        # 10. 更新任务记录
        job.status = 'completed'
        job.completed_at = datetime.now()
        job.train_samples = len(X_train)
        job.val_samples = len(X_val)
        job.test_samples = len(X_test)
        job.accuracy = metrics['accuracy']
        job.precision_val = metrics['precision']
        job.recall_val = metrics['recall']
        job.f1_score = metrics['f1_score']
        job.true_positives = metrics['true_positives']
        job.true_negatives = metrics['true_negatives']
        job.false_positives = metrics['false_positives']
        job.false_negatives = metrics['false_negatives']
        job.model_path = model_path
        job.config = {
            'algorithm': 'random_forest',
            'best_params': best_params,
            'cv_scores': cv_scores.tolist()
        }
        db.commit()

        logger.info(f"[Job {job_id}] 训练完成！")

    except Exception as e:
        logger.error(f"[Job {job_id}] 训练失败: {e}")

        # 更新任务状态为失败
        job.status = 'failed'
        job.completed_at = datetime.now()
        job.error_message = str(e)
        db.commit()

    finally:
        db.close()
```

---

## 模型管理

### 模型版本控制

**上传新模型版本**：

```python
@app.post("/api/model/upload")
async def upload_model(
    version: str,
    algorithm: str,
    accuracy: float,
    f1_score: float,
    model_size_mb: float,
    checksum: str,
    model_url: str,
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)
):
    try:
        # 检查版本是否已存在
        existing = db.query(ModelVersion).filter(
            ModelVersion.version == version
        ).first()

        if existing:
            raise HTTPException(status_code=409, detail=f"模型版本 {version} 已存在")

        # 创建新模型版本记录
        new_model = ModelVersion(
            version=version,
            algorithm=algorithm,
            accuracy=accuracy,
            f1_score=f1_score,
            model_size_mb=model_size_mb,
            checksum=checksum,
            model_url=model_url,
            is_active=True
        )

        db.add(new_model)
        db.commit()
        db.refresh(new_model)

        logger.info(f"新模型版本已上传: {version}")

        return {
            "success": True,
            "message": f"模型版本 {version} 上传成功",
            "data": {
                "id": new_model.id,
                "version": new_model.version,
                "deployed_at": new_model.deployed_at.isoformat()
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"上传模型失败: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
```

---

### 模型导出

**export_model.py**：

```python
import hashlib
import os
import joblib

def export_model(model_path, output_dir='exports'):
    """导出模型并生成校验和"""

    # 1. 加载模型
    model = joblib.load(model_path)

    # 2. 计算文件大小
    file_size_mb = os.path.getsize(model_path) / (1024 * 1024)

    # 3. 计算 SHA-256 校验和
    sha256 = hashlib.sha256()
    with open(model_path, 'rb') as f:
        while True:
            data = f.read(65536)  # 64KB 块
            if not data:
                break
            sha256.update(data)

    checksum = sha256.hexdigest()

    # 4. 复制到导出目录
    os.makedirs(output_dir, exist_ok=True)
    export_path = os.path.join(output_dir, os.path.basename(model_path))

    import shutil
    shutil.copy(model_path, export_path)

    logger.info(f"模型已导出:")
    logger.info(f"  路径: {export_path}")
    logger.info(f"  大小: {file_size_mb:.2f} MB")
    logger.info(f"  校验和: {checksum}")

    return {
        'path': export_path,
        'size_mb': file_size_mb,
        'checksum': checksum
    }
```

---

## 安全性

### 1. API 认证

```python
# 环境变量配置
ADMIN_API_KEY = os.getenv('ADMIN_API_KEY', 'your-secret-api-key')

# Header 验证
def verify_api_key(x_api_key: str = Header(...)):
    if x_api_key != ADMIN_API_KEY:
        raise HTTPException(status_code=401, detail="Unauthorized")
    return x_api_key
```

### 2. SQL 注入防护

使用 SQLAlchemy ORM 参数化查询：

```python
# ✅ 安全
user = db.query(User).filter(User.id == user_id).first()

# ❌ 不安全
user = db.execute(f"SELECT * FROM users WHERE id = {user_id}")
```

### 3. 数据验证

使用 Pydantic 验证输入：

```python
class UploadDataRequest(BaseModel):
    user_id: str
    label: str  # 'fall' or 'normal'
    source: str  # 'simulate' or 'riding'
    samples: List[SensorDataPoint]

    @validator('label')
    def validate_label(cls, v):
        if v not in ['fall', 'normal']:
            raise ValueError("label must be 'fall' or 'normal'")
        return v
```

### 4. CORS 配置

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # 生产环境应限制为特定域名
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## 性能优化

### 1. 数据库优化

**索引优化**：

```sql
-- 时间索引
CREATE INDEX idx_uploaded_at ON training_samples(uploaded_at);

-- 标签索引
CREATE INDEX idx_label ON training_samples(label);

-- 样本 ID 索引
CREATE INDEX idx_sample_id ON sensor_data(sample_id);
```

**批量插入**：

```python
# ✅ 高效
db.bulk_save_objects(sensor_data_list)
db.commit()

# ❌ 低效
for data in sensor_data_list:
    db.add(data)
    db.commit()
```

### 2. API 性能

**异步处理**：

```python
import asyncio

@app.post("/api/training/start")
async def start_training(db: Session = Depends(get_db)):
    # 创建训练任务
    job = create_training_job(db)

    # 异步执行训练
    asyncio.create_task(run_training_async(job.id))

    return {"job_id": job.id, "status": "pending"}
```

**缓存**：

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_latest_model_version():
    # 缓存最新模型版本
    pass
```

### 3. 训练优化

**并行处理**：

```python
# 随机森林并行
RandomForestClassifier(n_estimators=100, n_jobs=-1)

# GridSearchCV 并行
GridSearchCV(model, param_grid, n_jobs=-1)
```

**特征缓存**：

```python
# 缓存特征提取结果
features_cache_file = 'features_cache.pkl'

if os.path.exists(features_cache_file):
    features_df = pd.read_pickle(features_cache_file)
else:
    features_df = extract_features(data)
    features_df.to_pickle(features_cache_file)
```

---

## 常见问题

### 1. 数据库连接失败

**原因**：
- MySQL 未启动
- 连接字符串错误
- 防火墙阻止

**解决方案**：

```bash
# 检查 MySQL 状态
sudo systemctl status mysql

# 启动 MySQL
sudo systemctl start mysql

# 测试连接
mysql -u root -p -h localhost -P 3306
```

### 2. 训练数据不足

**建议**：
- 摔倒样本：至少 100+
- 正常样本：至少 1000+
- 平衡数据集

**数据增强**：

```python
from imblearn.over_sampling import SMOTE

# 使用 SMOTE 过采样少数类
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

### 3. 模型过拟合

**解决方案**：

```python
# 1. 减少模型复杂度
RandomForestClassifier(
    n_estimators=50,      # 减少树数量
    max_depth=5,          # 限制树深度
    min_samples_split=10  # 增加分裂样本数
)

# 2. 使用正则化（SVM）
SVC(C=0.1, kernel='linear')

# 3. 交叉验证
cross_val_score(model, X, y, cv=10)
```

---

## 总结

本文档详细介绍了 CycloSafe Python 后端的完整实现，包括：

- **FastAPI 应用**：RESTful API 服务
- **MySQL 数据库**：4 张表的设计和 ORM 模型
- **API 接口**：10+ 个接口（公开 + 管理）
- **机器学习**：数据清洗、特征提取、模型训练、评估
- **模型管理**：版本控制、导出、下载
- **安全性**：API 认证、SQL 防注入、数据验证
- **性能优化**：索引、批量操作、异步处理、并行训练

**关键特性**：
- 完整的训练数据管理
- 自动化的模型训练流程
- 版本化的模型部署
- 安全的 API 认证

**下一步**：
- [03-数据流和通信.md](./03-数据流和通信.md) - 前后端数据交互
- [04-部署指南.md](./04-部署指南.md) - 完整部署说明
- [05-开发指南.md](./05-开发指南.md) - 开发者指南

---

**文件位置**：`backend/`

**主要文件**：
- `main.py` - FastAPI 应用入口（462 行）
- `train_model.py` - 模型训练主脚本（600+ 行）
- `models.py` - SQLAlchemy 模型（73 行）
- `database.py` - 数据库连接
- `config.py` - 配置管理
